var store = [{
        "title": "EEG Setup",
        "excerpt":"TLDR; I’m getting started with biometric sensor data, namely EEG.   For the last 2 weeks I’ve been spending some time learning about EEG signals and biometrics signals more broadly. EEG signals are an interesting source of data for us because they have the power to tap into what might be going on in someone’s head - what’s their reaction to this advertisement? how engaging is this movie? and can we move one step closer toward a man-machine fusion, one that you can use to issue commands to type, text, send messages, etc. using just your mind?         The complete EEG setup   These are a couple of the types of questions that I’ll be attempting to answer in the coming weeks and months. There seems to be a growing interest in making better use of biometrics data, like Audi testing out the new autonomous A8. If we combine EEG data with other data sources such as video (e.g. facial recognition, visual stimuli), activity tracker data (e.g. GSR, heart rate, accelerometer) we can achieve interesting combinations. My hope is that machine learning, and especially deep learning, will take us beyond descriptive statistics. I see a lot of applications of EEG using well known features such as alpha wave detection to do fairly binary task (i.e. Move/don’t move, yes/no). What if we don’t know what patterns to look for in the data? That’s where deep learning excels. Deep learning may allow us to actually discriminate between discrete thoughts from moment to moment, learn those thought patterns, and then be able to take an action when it sees those learned patterns, such as controlling which direction you want to move in pac-man or something more complex like texting. In part, this is our motivation for exploring this somewhat mature technology (EEG) - deep learning’s ability to find patterns in places we previously thought impossible.  Finally, we find similarities between biometrics data and what you would find in an IoT/Sensor Streaming scenario and it’s nice to have some familiarity with the “art of the possible” as the partners always like to say.   If you’re interested in learning more about the setup and EEG signals more broadly, I’d recommend Chip Audette’s blog. I intend to pick up where this blog leaves off. Two other companies doing neat stuff with biometrics data are imotions.com and biopac.com.   This week I’m going to introduce the hardware we’ve chosen:      16 Channel OpenBCI Mark 4 Headset   Empatica E4 Wristband   UPS lost the E4 Wristband so I’m still waiting for that to arrive, but the OpenBCI Mark 4 Headset arrived in the mail and I want to recap what I learned while assembling it.   The OpenBCI Mark 4 Headset comes in a few different forms: assembled, unassembled, or self 3D printed. I chose unassembled and put the headset together myself. Here were a few things that I saw along the way.         Assembling the headset   The documentation page was super helpful for assembling the headset. I didn’t have all the tools they listed by I did just fine assembling the headset with a razor blade, some sandpaper, a screw driver, and super glue. The docs make it seem like you only need one super glue but I ended up needing 2 .14oz Loctite super glues. I’d also recommend gloves as you need to apply a lot of glue to this headset and you’re bound to get a bunch on your fingers. Have fun picking that off for the next couple hours.. I found out the hard way.         Superglue on my fingers   After painstakingly glueing everything together, you arrive at the electronics. It’s fairly obvious how the daisy board (the smaller board) sits on top of the main board (the bigger board) and how the USB Bluetooth receiver works. The trickier part, however, is pinning down all those wires running from the electrodes back to the cyton boards; in my case, 16 channels. It was also unclear why I was getting a “RAILED” error message for the 9-16 channels from the OpenBCI GUI when I plugged everything in and tried to stream EEG waves. It turns out that you need to use the splitter wire provided in the Cyton board box to connect the “SRG ground” signal to both the main board and the daisy board. After that fix, I was up and running.   I promptly recorded about a minute of 16 channel + accelerometer data if anyone is interested in seeing that. I’ve attached a sample data set below where I start normal (eyes open), then close my eyes (should see alpha waves), clench my jaw (lots of artifacts), and then shake my head side-to-side and front-to-back (see what the accelerometer picked up). My hypothesis is that kmeans might be able discriminate between the ~5 different actions above.         Assembling the electronics   Next up, getting into the code in the following post.  ","categories": ["Biometrics"],
        "tags": ["biometrics"],
        "url": "/biometrics/eeg-setup/",
        "teaser": "/assets/images/eeg/eeg_headset.jpg"
      },{
        "title": "KMeans Clustering Results",
        "excerpt":"TLDR; KMeans can be used to discriminate between different time segments within an EEG recording.   Last time we left off with our motivation for exploring EEG data given that machine learning, and deep learning specifically, might do a better job identifying patterns in the EEG data. As a first exercise, I would like to accomplish two things: 1) do some exploratory data analysis on EEG data to better understand this data format, and 2) determine what features can be created and what they might be useful for. KMeans is a great algorithm for separating data into clusters when you don’t have labels. There are a number of assumptions to look out for and cluster evaluation is more of an art than a science but I’ll try to provide some color on this as we go through the results.   To recap, I recorded about a minute of 16 channel + accelerometer data. This data is a recording of sitting normally (eyes open), eyes closed (should see alpha waves), jaw clenching (lots of artifacts), and shaking my head side-to-side and front-to-back (see what the accelerometer picked up). My hypothesis is that KMeans might be able discriminate between the ~5 different actions above.         Raw 16 channel EEG data, which shows relative microvolts over time (relative to bias electrode)   Preprocess the data   The data preprocessing steps can be overwhelming when first starting out with EEG data so let’s take it one step at a time and look at some code along the way. For the complete code, check out my script here.   signal_data = df['Channel_8']  # https://github.com/chipaudette/EEGHacker/blob/master/Data/2014-10-03%20V3%20Alpha/exploreData.py # filter the data to remove DC hp_cutoff_Hz = 1.0 print(\"highpass filtering at: \" + str(hp_cutoff_Hz) + \" Hz\") b, a = signal.butter(2, hp_cutoff_Hz/(sampling_rate / 2.0), 'highpass')  # define the filter clean_signal = signal.lfilter(b, a, signal_data, 0) # apply along the zeroeth dimension  # notch filter the data to remove 60 Hz and 120 Hz interference notch_freq_Hz = np.array([60.0, 120.0])  # these are the center frequencies for freq_Hz in np.nditer(notch_freq_Hz):  # loop over each center freq     bp_stop_Hz = freq_Hz + 3.0*np.array([-1, 1])  # set the stop band     print(\"notch filtering: \" + str(bp_stop_Hz[0]) + \"-\" + str(bp_stop_Hz[1]) + \" Hz\")     b, a = signal.butter(3, bp_stop_Hz/(sampling_rate / 2.0), 'bandstop')  # create the filter     clean_signal = signal.lfilter(b, a, clean_signal, 0)  # apply along the zeroeth dimension   One of the preprocessing steps requires bandpass filtering the raw EEG signals. There isn’t much to see below 1Hz in the brain and this range tends to contain a lot of noise so we’re going to filter that out. We’re also going to notch filter out 60Hz and its harmonic at 120Hz because 60Hz is the utility frequency used for electricity in the United States.   Once we’ve removed those frequency bands, we do a spectrogram plot to see what the recording looks like. A spectrogram plot is great because it conveys 3 pieces of information: 1) time along the x-axis, 2) frequency (Hz) along the y-axis, and 3) the magnitude of the signal from the color. What’s neat here is that it’s visually intuitive once you get familiar with it, and more interestingly, we can probably use these plots to conduct our deep learning experiments. For example, if you look at some of the speech-to-text deep learning architectures, you’ll see that they use spectrogram time slices that get fed to a convolutional recurrent neural network. In other words, a convolutional neural network runs over each fractional second spectrogram and the activation vector gets passed on to a recurrent neural network that translates speech-to-text. My team has worked with Baidu’s architecture so this experience should bootstrap some of our coding.   NFFT = int(sampling_rate*1) # length of the fft overlap = NFFT - int(0.25 * sampling_rate)  # three quarter-second overlap  fig = plt.figure(figsize=(30, 9))  Pxx, freqs, bins, im = plt.specgram(clean_signal,NFFT=NFFT,\\                                     Fs=sampling_rate, noverlap=overlap, \\                                     cmap=plt.get_cmap('viridis'))  Pxx_perbin = Pxx * sampling_rate / float(NFFT)  #convert to \"per bin\" # need to better understand that  # reduce size # Pxx_perbin = Pxx_perbin[:, 1:-1:2]  # get every other time slice # bins = bins[1:-1:2]  # get every other time slice  plt.pcolor(bins, freqs, 10*np.log10(Pxx_perbin), cmap=plt.get_cmap('viridis'))  # dB re: 1 uV plt.clim(-15,15) plt.xlim(bins[0], bins[-1]) plt.ylim(0,40) plt.xlabel('Time (sec)',fontsize=22) plt.ylabel('Frequency (Hz)',fontsize=22) plt.xticks(fontsize=22) plt.yticks(fontsize=22) plt.show()        Spectrogram   The above code plots a nice looking spectrogram of the ~83 second recording. In the first 20 seconds you can see what my normal state looks like followed by ~25 seconds of eyes closed. After that, you can see the artifacts associated with jaw clenching followed by head shaking side-to-side and front-to-back.   KMeans attempt 1   From here, we should just take a naïve pass at using the raw EEG signal plus the accelerometer data to see what KMeans comes up with using k=4 clusters.   scaler = preprocessing.StandardScaler() training_data = scaler.fit_transform(df[data_cols]) cluster = KMeans(n_clusters=4) results = cluster.fit_predict(training_data) df['cluster_number'] = results  start_index = 0 duration=int(len(df.iloc[start_index:])/sampling_rate) print(\"{} seconds of recording\".format(duration)) df.iloc[start_index:][\"cluster_number\"].plot(figsize=(30,9)) plt.xlabel('Timestamp',fontsize=22) plt.ylabel('Cluster Number',fontsize=22) plt.xticks(fontsize=22) plt.yticks(fontsize=22)        Raw K-means results   KMeans is probably heavily relying on signal magnitude to drive cluster assignment   The above plot shows cluster assignment over time of each data point (1/256th of a second given a sampling rate of 256Hz). One neat observation here is that this first pass does a pretty good job breaking up the jaw clenching vs headshaking sections. It also appears to be making cluster assignments according to the back and forth movement of my head at the end of the recording. The only downside here is that it’s treating the whole first portion of the recording as one cluster and yet we know that there should an “eyes closed” cluster. What we need to do now is find the right feature to break this big first cluster up. Let’s move from the time domain to the frequency domain and summarize the frequency with the maximum magnitude, the mean frequency, the sum of the magnitude, and the power of the signal (i.e. mag**2) for a sliding window along the recording.   Before we move on, you may be wondering what preprocessing.StandardScaler() does. It is used to scale our features so that they have a mean of 0 and unit variance (variance of 1). This makes features with different scales comparable. KMeans is a (within cluster) variance minimization algorithm and if one of your dimensions is in dollars and one is yen, you’re likely to see clusters driven by yen just because there will appear to be more variance in that dimension. In our case, the raw EEG data and the accelerometer data have different scales so if we didn’t standardize the data, the raw signal would impact the final clustering results more than the accelerometer would.   Try generating a frequency summary for each timestep (timestep adjustable) for 1 channel of EEG Data (Channel 8/O2)   NB: I’m going to start with one channel for two reasons: 1) ease of initial analysis, and 2) if we create 4 features for each of the 16 channels, then we’ll have 64 features per timestep, which means we’ll need to start thinking about PCA due to the curse of dimensionality.   n = len(clean_signal) # length of the signal k = np.arange(n) # array of timesteps in observations T = n/float(sampling_rate) # number of sampling cycles  freqs = k/np.float(T) # two sides frequency range  freqs = freqs[range(n/2)] # one side frequency range # due to nyquist frequency  lower_freq = 0. upper_freq = 40. mask = np.logical_and(freqs &gt;= lower_freq, freqs &lt;= upper_freq)  Y = np.fft.fft(clean_signal)*2/n # fft computing and normalization # why normalize? again a result of the nyquist frequency Y = Y[range(n/2)] # only taking half of the outputs Y = np.abs(Y) # take the magnitude of the signals  # filter down to frequencies of interest Y = Y[mask] freqs = freqs[mask]  fig = plt.figure(figsize=(16, 9)) plt.plot(freqs, Y) plt.xlabel(\"Frequency (hz)\",fontsize=22) plt.ylabel(\"Magnitude (uV)\",fontsize=22) plt.xticks(fontsize=22) plt.yticks(fontsize=22) plt.show()        Frequency domain plot   What we see above is a shift from the time domain to the frequency domain. Imagine the time domain as a bunch of sinusoid waveforms (because that’s what it is!) and what the frequency plot is doing is measuring the magnitude of the sinusoids at different oscillation frequencies. The result is a summary of the time domain signal. Instead of looking at the entire recording, we can look at smaller chunks of the recording and generate features that summarize that window. In my case, I used one second recordings and generated 4 features: max magnitude frequency, mean frequency, sum of magnitude, and power (i.e. mag**2)]. We can then use these features for clustering.   def fft(data, sampling_rate=256, lower_freq=0., upper_freq=40.):     n = len(data) # length of the signal     k = np.arange(n) # array of timesteps in observations     T = n/float(sampling_rate) # number of sampling cycles      freqs = k/np.float(T) # two sides frequency range      freqs = freqs[range(n/2)] # one side frequency range # due to nyquist frequency      mask = np.logical_and(freqs &gt;= lower_freq, freqs &lt;= upper_freq)      Y = np.fft.fft(data)*2/n # fft computing and normalization # why normalize? again a result of the nyquist frequency     Y = Y[range(n/2)] # only taking half of the outputs     Y = np.abs(Y) # take the magnitude of the signals      # filter down to frequencies of interest     Y = Y[mask]     freqs = freqs[mask]          return Y, freqs  def descriptive_stats(freq_data, mag_data):     max_mag_freq = freq_data[mag_data.argmax()]     mean_freq = np.sum((freq_data * (mag_data/np.sum(mag_data))))     mag = np.sum(mag_data)     power = np.sum(mag_data**2)     return [max_mag_freq, mean_freq, mag, power]  # generate summary statistics plotting = False output = [] step_size_seconds = 1. step_size = step_size_seconds*sampling_rate for i in range(0,len(clean_signal),int(step_size)):     Y, freqs = fft(clean_signal[i:i+int(step_size)],lower_freq=1.)     descr = descriptive_stats(mag_data=Y, freq_data=freqs)     output.append(descr)          if plotting:         fig, ax = plt.subplots()         ax.plot(freqs, Y)         plt.xlabel(\"Frequency (hz)\")         plt.ylabel(\"Magnitude (uV)\")         plt.show()  The code loops through the entire recording 1 second at a time and generates the 4 features I mentioned above. Let’s look at the max magnitude frequency to determine if can break up the initial ~45 seconds of the recording.   training_data = scaler.fit_transform(data) cluster = KMeans(n_clusters=4) results = cluster.fit_predict(data)  fig, ax = plt.subplots(figsize=(30,9)) ax.plot(range(0,len(results)), results) plt.xlabel('Time (sec)',fontsize=22) plt.ylabel('Cluster Number',fontsize=22) plt.xticks(fontsize=22) plt.yticks(fontsize=22) plt.show()        Max magnitude frequencies   It appears that this feature isn’t perfect for clustering everything but it DOES start to carve out the eyes closed portions  Again, the above plot shows cluster assignment over time of each data point (1/256th of a second given a sampling rate of 256Hz). I’m sure my alpha signal wasn’t perfectly steady so I’m not surprised there are multiple cluster assignments between seconds 20-45. Unfortunately, this feature can’t really tell the difference between jaw clenching and head shaking, so we should look at finding that feature next.         Magnitude feature   It appears that this feature isn’t perfect for clustering everything either but it DOES start to break up the jaw clenching and head shaking.  This plot is probably exposing some of the assumptions of the kmeans algorithm, namely that each cluster ought to have the same variance. Since the variance of the magnitude feature within the jaw clenching and head shaking portion of the segment is so high, it works on minimizing variance within those clusters, while the lower variance normal and eyes closed segments look like 1 blob that doesn’t need to be broken up. The latter segment dwarfs the first 45 seconds in terms of variance.   We achieved what we set out to do: 1) exploratory data analysis on EEG data to better understand this data format, and 2) determine what features can be created and what they might be useful for.   Again, for the complete code please checkout my script here.  ","categories": ["Biometrics"],
        "tags": ["biometrics"],
        "url": "/biometrics/kmeans/",
        "teaser": "/assets/images/kmeans/spectrogram.png"
      },{
        "title": "Association for Computational Linguistics (ACL) Conference",
        "excerpt":"TLDR; I recently attended the ACL conference, where I learned about dialog systems and NLP for robotics.   This past week I attended the conference for the Association for Computational Linguistics (ACL). It is one of the premier venues for NLP research and naturally all of the big names were in attendance (e.g. Google, Facebook, etc.). This was my first time attending so I can’t speak about previous years but it was interesting to see such a strong presence from the Chinese tech firms (e.g. Baidu, Tencent, Alibaba, etc.) not to mention the representation from the Chinese research arms of the US tech firms (e.g. Microsoft Beijing, etc.). China has a deep pool of technical talent and they’ve made their goals clear, not mention comments like these from Andrew Ng, “China is very good at inventing and quickly shipping AI products.”   Beyond that, it was my first truly academic conference and it was exciting to learn more about how the community works (e.g. double blind reviewing, presentations of very incremental developments, etc.). I’d like to highlight a few of my key learnings below.   For context, my team has done a lot of work with vector representations but we still have much to learn. In the neural realm, we’ve worked with word2vec/GloVE embeddings, LSTMs for classification tasks, autoencoding using seq2seq architectures, information retrieval chatbots, text summarization, and unsupervised clustering of word/document representations. With that in mind, we haven’t done a tremendous amount with knowledge bases, slot filling in dialogue systems, or grounding language to actions for robotics. I’m sure others in attendance got a lot from the very technical updates on new attention mechanisms or updated gating structures in RNN cells, but I was learning about broader brushstrokes.   Paper Review   One paper that was fairly eye opening was End-to-End Task-Completion Neural Dialogue Systems. I attended Vivian Chen’s workshop on dialogue systems on Sunday before the conference started. The paper describes an end-to-end goal-oriented chatbot that helps a user book movie tickets. The idea is that the user can ask questions and naturally specify what they’re looking for (e.g. What time is the new Star Wars movie playing at Cinemax?). The authors’ approach uses a combination of Supervised + Reinforcement Learning.         Sample conversation   The first module is a language understanding (LU) model, an LSTM, that both predicts intent (e.g. greeting, request info, inform the bot, etc.) and slots (e.g. place, time, movie, number of tickets, etc.), which are used to query the relevant database (e.g. a database of movie info). The training for the LU is done in a supervised fashion, relying on prelabeled sequences. In other words, the LSTM learns the mapping from the text sequence to the intention and slots.   The LU module outputs intent and slots (i.e. symbolic representations) that are then passed to the dialogue management (DM) module. The DM module is responsible for querying the database using the intent and slot information, summarizing the current state (i.e. all previous user utterances, previous bot utterances, etc.), and learning a policy that takes actions based on that state (e.g. ask the user for more info). The DM module is warm-started in a supervised fashion and continues learning the policy to take actions using reinforecement learning.   They employ a user simulator, which is a proxy for a true user and is a way to augment the training data. The simulated user uses a natural language generation (NLG) module, which is a sequence-to-sequence module that takes requests/actions as input and outputs a sketch sentence. They employ some heuristics to clean this output sentence up. Again, existing training data is used to train this sequence-to-sequence model in a supervised fashion. To be sure, the output of the user simulator is what is sent to the LU module described above.         Chatbot architecture   The authors also add in intent and slot level noise to the text sequences to test how robust the model is noisy input (i.e. the real world). Naturally, as you increase the noise, the performance of the system is likely to degrade, which is largely what we observe in the success rate of the conversation. The conversation outcome is binary and is marked as a success if (1) a movie is booked, and (2) the movie satisfies the users constraints.         Beyond this paper, it will be interesting to see how RL plays a role in future learning systems, especially in situations where there is not necessarily labeled training data or where the system needs is allowed to continue improving through interactions with the real world.  ","categories": ["NLP"],
        "tags": ["NLP","conferences"],
        "url": "/nlp/ACL/",
        "teaser": "/assets/images/nlp/architecture.png"
      },{
        "title": "Entrainment with Visual Stimuli",
        "excerpt":"TLDR; I created a strobe video that my brain can entrain with.   When learning a new skill, especially with programming and hardware, I find it best to start with small experiments, confirm my understanding of the results, and build on from there. Over the past month or so, I set out to detect a well defined phenomemon in my brain using the EEG headset - entrainment with visual stimuli. If this experiment is a success, we will better understand the principles underlying this rather complex source of data - EEG signals. From there, we can move on to machine learning with EEG/biometrics data, which is far more unchartered territory.   Brainwave entrainment is when your brainwaves synchronize with visual, audio, or tactile stimulus. The goal for this experiment was to watch a video on a computer screen that flashes essentially a strobe light at a defined frequency such as 10 or 12hz and then be able to detect the entrained brainwaves in the EEG recording. If we can detect a signal in our brainwaves, we can use it to issue commands. The commands will be limited to a handful of actions (e.g. turn a robot left or right) but we’ll be controlling something with our mind. To be sure, the idea here is that if you have multiple signals on the screen such as 10 or 12hz, each of which corresponds to an action, you can issue an action simply by looking at the corresponding frequency!         A colleague was a real sport and volunteered to try entrainment..   A word of caution before proceeding, visual entrainment does pose the risk of seizures so if you’re epileptic or you don’t respond well to flashing lights, DO NOT TRY THIS EXPERIMENT.   With that out of the way here’s our outline:     Create the visual stimuli - video recordings of different frequency strobe lights, which is what we’re discussing today   Record the EEG signal while watching the videos   Analyze the data and try to detect the entrained signal   Create the visual stimuli   I’m sure there are a number of ways of creating a strobe light video and I did spend some time Googling ways do this in programs like iMovie but the path forward wasn’t clear to me. The more I thought about it, I figured Python and namely Matplotlib could help. After finishing the script, a colleague mentioned MoviePy, which is probably worth checking out though I’m not sure if it would solve my strobe light problem. In any event, Jake Vanderplas is one of the best teachers out there on the internet and I checked out one of his posts to better understand the Matplotlib Animation class, specifically the FuncAnimation class. Needless to say, getting this all to run smoothly will take time and I hope I left my code well commented enough to be helpful to anyone else attempting this. The one key concept I’ll highlight here is the generator function - the crux of getting this thing to run. Every time this generator function gets called, it yields a new frame that gets added to the video and it’s up to you to decide what goes into that frame!   def frame_gen(frame_count=frame_count, frame_rate=frame_rate, desired_hz=desired_hz):     \"\"\"generates a new frame every time it is called, keeping track of where the video is across different     frequencies and when to switch from black to white\"\"\"     closest_approx = approximate_hz(desired_hz=desired_hz, frame_rate=frame_rate)     hz_switch_point_frame = hz_switch_point(frame_count=frame_count, desired_hz=desired_hz)          current_frequency = -1 # zeroeth index of desired_hz (gets incremented by one in modulus in first run)     on = True     i = 0     while True:         if i % hz_switch_point_frame == 0:             current_frequency +=1 # incremented by one in the first run         # black         if i % (closest_approx[current_frequency]) == 0:             on = not on             np_array = np.ones((height, width),dtype=int) * on # switch from black to white             i += 1             yield np_array         # white         else:             i += 1             yield np_array  Let’s take it one line at time.   The approximate_hz function takes in a list of desired frequencies and returns a list containing switch points. In other words, to approximate a 10hz frequency, you would need to switch from black to white every 6 frames (assuming 60 frames per second (FPS)). I learned that you actually can’t show a 9hz signal perfectly using a 60FPS monitor. In fact, the only frequencies you can show are those that divide evenly into 60. Think about it: 60 / 9 = 6.667. You can’t change the strobe every 6.667 frames. You can only switch every 6 or 7 frames, which means your effective frequency is either 10 or ~8.6hz, respectively. Now imagine that you had a monitor with a 120FPS refresh rate. What would be the closest frequency that would approximate 9hz? Answer: ~9.2hz, which corresponds to a switch every 13 frames. Incidentally, the faster your refresh rate is, the better you can appoximate the frequency. I found this fascinating but I digress. The point here is that you shouldn’t be surprised if your entrainment shows up at an unexpected frequency (if you use a frequency that doesn’t divide evenly into your refresh rate) when you analyze the data.   The hz_switch_point_function keeps track of which frequency the generator is working on. If you want a movie that’s 60 seconds long across 10 and 12 hz, this means you will switch from 10 to 12hz at the 30secondsx60FPS = 1,800th frame.   Finally, the while loop is the main part of the generator function. The i variable is a simple incremeter that keeps track of how many times the function has been called. If i modulus switch point == 0, it’s time to switch from black to white! Any other time that modulus != 0, simply yield the existing numpy array. I’m conveniently taking advantage of the fact that matplotlib treats zeros and ones as black and white respectively using cmap=”gray”.   The final thing I’ll note here is that I found FFMPEG to be the fastest movie writer. You can try ImageMagickWriter but it was significantly slower because it writes all the frames out to disk individually and then stitches them together.   If you spend a little time reading the code it’ll become clearer. There are two scripts worth checking out: 1) entrainment_891011hz_full.ipynb outputs a full screen strobe, and 2) entrainment_1012hz_split_screen.ipynb outputs a split screen strobe.   In the next post, we’ll cover the analysis of the entrained EEG signal.  ","categories": ["NLP"],
        "tags": ["biometrics"],
        "url": "/nlp/entrainment/",
        "teaser": "/assets/images/entrainment/entrainment.jpg"
      },{
        "title": "Detecting Entrained Signals - Part I",
        "excerpt":"TLDR; I detected entrained brain waves from the EEG headset.   Continuing from the last post, I’ll recap what we’re working toward and the topic of this post. The goal for this experiment was to watch a video on a computer screen that flashes essentially a strobe light at a defined frequency such as 10 or 12hz and then be able to detect the entrained brainwaves in the EEG recording. If we can detect a signal in our brainwaves, we can use it to issue commands.   Here’s our outline:     Create the visual stimuli - previous post: Entrainment with Visual Stimuli   Record the EEG signal while watching the videos - today’s post            Video used for this analysis       Video that can be used for issuing commands           Analyze the data and try to detect the entrained signal - visual part of today’s post   Record the EEG signal while watching the videos   First, I’ll note that entrainment is not the most comfortable thing. You have to look at a blinking light and once the frequency gets above 10 hz it’s pretty trippy. It does seem to get better with time but generally, the goal is just to relax and watch the screen! Relax your thoughts and your gaze and soon you’ll be seeing patterns in the screen, which is a strong indication you’re entrained with the signal.   For this post, I’ll be analyzing the results of watching the 100 second long full screen video. When I created this video, I wanted to have 25 seconds of 8, 9, 10, and 11 hz respectively. Due to reasons explained in the previous post, I ended up with a video that has 50 seconds at 8.6hz, 25 seconds at 10hz, and 25 seconds at 12hz. This is fine, I just had to remind myself of that when I analyzed the results.   Analyze the data and try to detect the entrained signal        You can spot the entrained signals clear as day!   Since our ultimate goal is to be able to detect the signal of interest real time, I’ll focus on how we can generate this spectrogram and detect the signal using something called the Short Time Fourier Transform (STFT). I’ll explain some code below for illustration purposes. Once we understand the principles involved, we can move on to calculating the Fourier Transform for short time segments with more control.   nperseg=1024 noverlap=int(nperseg/1.05) nfft=nperseg plt.figure(figsize=(30,9)) # boundary=None got rid of the strips at the start and the end # zero padding (nfft) interpolates and makes the spectrogram look smoother # high noverlap gives you smooth temporal resolution f, t, Zxx = signal.stft(raw_signal, sampling_rate, nperseg=nperseg, noverlap=noverlap, nfft=nfft, boundary=None) plt.pcolormesh(t, f, 10*np.log10(np.abs(Zxx)),cmap=plt.get_cmap('jet')) plt.title('STFT Magnitude', fontsize=22) plt.ylabel('Frequency [Hz]', fontsize=22) plt.xlabel('Time [sec]', fontsize=22) plt.clim(-10, 5) plt.ylim(0,20) plt.xticks(fontsize=22) plt.yticks(fontsize=22) plt.show()  Let’s take it one line at time.   nperseg = 1024 is actually probably longer than necessary - it’s 4 seconds, given a sampling rate of 256hz. This is the length of the signal going into the Fourier Transform. Again, the Fourier Transform decomposes a signal into its component frequencies and their respective magnitudes. noverlap specifies the number of overlapping points. In other words, it becomes the step size. If your signal length is 1024 and your overlap is ~975, then your effective step size is ~49. 49samples/256samples/second is ~.2 of a second or 1/5th of one second. You can imagine your signal segment that is getting passed to the Fourier Transform as a tape reel. At each step, you update your tape reel with the 49 newest values (on the right side of the reel) and shift the older values to the left by 49. So that’s it! Each time you compute the STFT, you’re using a window in time that you define! Finally, the nfft parameter can be greater than or equal to the nperseg and it allows you to zero pad the segment. Ignoring zero padding is fine for now.   Let’s unpack what the signal.stft function returns. The first return value is f for frequencies. These are the component frequencies of your signal (e.g. 1hz, 2hz, 3hz, etc.). t is for time segments, which correspond to the time at which the FFT was taken (e.g. 0.2 seconds, 0.4 seconds, etc.). In our spectrogram above, we see a new column every ~0.2 seconds, corresponding to the step size of ~0.2 seconds. Finally, Zxx is an array of magnitudes of signals for each frequency for each timestep. Rows correspond to a specific frequency (refer to f to know which row indices correspond to which frequences), while columns correspond to a given timestep - it’s the same as you see in the spectogram! The spectrogram is colored based on the magnitude of the signal at different frequencies and timesteps.   Matplotlib can be tricky if you’re not super familiar with it. plt.colormesh gives you the spectrogram, while the 10*np.log10 of our signal gives the power decibels to scale the signal to a more reasonable range. I happen to like plt.get_cmap(‘jet’)) but you can experiment with other color maps. The only other thing worth noting (and isn’t self-explanatory) is plt.clim. I had to play around with these color ranges until I was pleased with the way the spectrogram looks.   So there it is! I think I’m going to stop here and continue explaining how to analyze this signal in the next post. In the next post, we’ll move beyond visual recognition of entrainment and move into a more rigorous algorithmic approach to signal detection.  ","categories": ["Biometrics"],
        "tags": ["biometrics"],
        "url": "/biometrics/detecting-entrainment-part-1/",
        "teaser": "/assets/images/entrainment/spectrogram.png"
      },{
        "title": "Detecting Entrained Signals - Part II",
        "excerpt":"TLDR; I detected entrained brain waves from the EEG headset.   Continuing from the last post, I’ll recap what we’re working toward and the topic of this post. The goal for this experiment was to watch a video on a computer screen that flashes essentially a strobe light at a defined frequency such as 10 or 12hz and then be able to detect the entrained brainwaves in the EEG recording. If we can detect a signal in our brainwaves, we can use it to issue commands.   Here’s our outline:     Create the visual stimuli - previous post: Entrainment with Visual Stimuli   Record the EEG signal while watching the videos - partially covered in the previous post: Detecting Entrained Signals - Part I            Video used for this analysis       Video that can be used for issuing commands           Analyze the data and try to detect the entrained signal - algorithmic part of today’s topic            visual portion covered in the previous post: Detecting Entrained Signals - Part I           Analyze the data and try to detect the entrained signal        You can define a threshold. Above that threshold, we can say the signal has been detected.   What you see in the graphic above is a 12hz cross section from the STFT output. Let’s look at the code that generated this plot.   def freq_filter(t, freqs, power, min_freq=10.0, max_freq=10.5, plot=True, threshold=3, vertical_plots=None, height=None):     \"\"\"Allows you to filter down to a frequency band and measure the mean signal strength.      Optionally plot results.\"\"\"     condition = (freqs &lt;= max_freq) &amp; (freqs &gt;= min_freq)     indices = np.where(condition)     power = np.abs(power)     filtered_signal = np.mean(power[indices], axis=0)     signal_length = len(filtered_signal)     threshold_line = signal_length*[threshold]     if plot:         plt.figure(figsize=(16,9))         plt.title('Frequency Range: {}-{}hz'.format(min_freq, max_freq), fontsize=22)         plt.ylabel('uVrms', fontsize=22)         plt.xlabel('Time [sec]', fontsize=22)         plt.xticks(fontsize=22)         plt.yticks(fontsize=22)         plt.plot(t, filtered_signal)         plt.plot(t, threshold_line)         if vertical_plots:             for val in vertical_plots:                 plt.plot([val]*height, range(height))         plt.show()     return filtered_signal  Building off of the last post, we’ll pass the time values, t, frequencies, and the power array (called Zxx in the last post) as well as a frequency range to narrow our results down. If we’re interested in looking at the signal strength in the 12hz range, we should probably look somewhere between 11.75-12.25hz. By taking advantage of numpy’s indexing functions such as np.where, we can drill down to the cross section of interest and take the average signal strength through time. Intuitively, we would expect the 12hz signal strenth to increase when we are entraining with a 12hz frequency.   From there we can plot the results and add in a couple extra lines to give us a visual sense of a plausible threshold. Perhaps we can define a threshold, above which we say the signal has been detected, below which we say the signal has not been detected. You can play around with the threshold - and therein lies the issue.   Due to cross-session differences (i.e. each time you put the EEG headset on), the threshold changes, likely due to the varying impedence on the electrodes. As such, it would be good if we could find a way to stabilize this threshold. One approach that came to mind was to normalize this signal by some other base signal such as the mean signal between 2-20hz. That mean of that range should basically just be noise. I’m literally creating a signal-to-noise ratio. Normalizing leads to far more stable results, and even improves our precision and recall scores quite dramatically for the 8.6hz and 10hz signal.         Normalized 12hz signal.   What you see in the graphic above is a 12hz cross section from the STFT output. Below the main plot, I included a cropped cross-section from spectrogram to drive the point home. The red in the spectrogram corresponds to the greater uVrms values in the plot above. Now let’s look at the code that generated this plot.   def plot_normalized_signal(t, freq, filtered_signal, normalizing_signal, threshold=1, vertical_plots=None, height=None):     \"\"\"Normalize the signal detection by another signal, which makes the     threshold more robust to individual and cross session differences. Plot the results.\"\"\"     plot_signal = filtered_signal / normalizing_signal     signal_length = len(plot_signal)     threshold_line = signal_length*[threshold]     plt.figure(figsize=(16,9))     plt.title('Normalized Signal: {}-{}hz'.format(freq, freq), fontsize=22)     plt.ylabel('Filtered Signal / Baseline', fontsize=22)     plt.xlabel('Time [sec]', fontsize=22)     plt.xticks(fontsize=22)     plt.yticks(fontsize=22)     plt.plot(t, plot_signal)     plt.plot(t, threshold_line)     if vertical_plots:         for val in vertical_plots:             plt.plot([val]*height, range(height))     plt.show()     return plot_signal  This function is almost exactly the same as the one earlier in the post except this time the function takes a pre-filtered signal, and a normalizing signal in as arguments.         Precision and recall results for a straight 10hz and normalized 10hz entrainment signal.   So how good are we doing? We’re able to detect our signal based on our naïve threshold approach. We can more rigorously evaluate our precision and recall metrics for the different signals. I was pleased to see the 10hz results improve so dramatically after using the normalizing technique described above. The rough idea here is that if we increase the threshold, we would expect precision to increase at the expense of recall. In other words, we’re being more selective about our detection criteria and thus more accurate in our predictions but the downside is a high false negative rate. On the flipside, lowering the threshold will result in higher recall and more false positives.   With all of this in place, we are now ready to run some live experiments. Let’s say we can detect 10 and 12hz with high fidelity, what should we do once we detect those signals? Turn the TV on? Issue commands to a robot? Move a CAT across the screen?!?! This wouldn’t be the internet if we didn’t do something with a cat. Take a look at the next post.  ","categories": ["Biometrics"],
        "tags": ["biometrics"],
        "url": "/biometrics/detecting-entrainment-part-2/",
        "teaser": "/assets/images/entrainment/12hznormalized.png"
      },{
        "title": "BCI with Entrainment",
        "excerpt":"TLDR; Brain control interface (BCI) app using entrainment.   This is the final post on entrainment, which is a culmination of the work explained in creating a visual stimulus, entrainment analysis part I, and entrainment analysis part II. As a final recap, the goal for this experiment was to watch a video on a computer screen that flashes essentially a strobe light at a defined frequency such as 10 or 12hz and then be able to detect the entrained brainwaves in the EEG recording. If we can detect a signal in our brainwaves, we can use it to issue commands.   Now that we can detect a 10 and 12hz signal with reasonably high fidelity, we can create a quick demo to demonstrate our new brain computer interface (BCI). A colleague suggested making something move across the screen so that’s what I went with. At a high level, I created a Flask app that handles streaming EEG data, and based on the signal that it detects, it moves yet another colleague’s CAT across the screen to the left or to the right. I could spend days tweaking this to make it better but the important thing is that it works and we learned a tremendous amount about the dynamics of the EEG signal in the process. After confirming our setup, we’re ready to move on to some machine learning experiments, but I digress.         Loosely speaking, these are the components of the app. Viewers might argue that the Flask app is the server side but I've got it in the middle to drive home the point of asynchronous communication.   To summarize, the EEG headset is connected my local computer. It streams data to Python/Flask. I run an FFT on small segments (remember the tape reel analogy?) and try to detect the signal that I’m looking for. If I detect it, I send a signal to the client side visualization and move the cat using javascript/jquery/html.   In order to make this work, I had to use a websocket to handle the streaming of information from the server (my local machine running the Flask app) to the client (Chrome). The Flask-socketio package was pretty intuitive and it’s documentation was solid. In the process of creating this app, I learned an incredible amount about the canonical client-server architecture and how you ideally want to offload some processes to the client side (say I productionized this, it would be paramount that I could capture the EEG steam in javascript on your machine, not mine).   I also now better appreciate the challenge of trying to run multiple things simultaneously (e.g. stream the EEG, process the signal, interact with the client). Streaming, processing, and interacting with the client really need their own threads and even more than that, you need to ensure that long-running processes like processing an EEG signal don’t hog all the CPU time, which is where sleeper functions come in. At first, my app handled one thing at a time - streaming, processing, or interacting with the client - but not all of them! After employing threading and sleeper functions, these problems went away.   As I’ve mentioned before, toy problems help you understand something before you go for a hail mary. The first app I created didn’t even bother with the EEG headset. Rather, I used an audio signal to nail the real-time streaming and visualization aspects before adding in the complication of an IoT device and noisy data source. The dynamics are exactly the same. Audio comes in a waveform in the same way that an EEG signal does, so if we can detect a frequency from an audio stream, we can do the same for an EEG stream just by simply swapping out the stream and signal we’re looking for. You can even run that toy example on your machine by checking out the code in my github repo.   Our tutorial wouldn’t be complete without looking at some code. I want to show you the workhorse functions of the app.   def freq_filter(freqs, power, min_freq=10.0, max_freq=10.5):     \"\"\"Allows you to filter down to a frequency band and returns the average.\"\"\"     condition = (freqs &lt;= max_freq) &amp; (freqs &gt;= min_freq)     indices = np.where(condition)     power = np.abs(power)     filtered_signal = np.mean(power[indices], axis=0)     return filtered_signal  def fft(data):     \"\"\"Fast Fourier Transform of the signal and emission to the client side based on the signal detected\"\"\"     data = data * np.hanning(len(data)) # smooth the FFT by windowing data     fft = abs(np.fft.fft(data).real) / fft_size     fft = fft[:int(len(fft)/2)] # keep only first half due to the nyquist frequency     freq = np.fft.fftfreq(len(data),1/float(sample_rate))     freq = freq[:int(len(freq)/2)] # keep only first half due to the nyquist frequency     freq_peak = freq[np.where(fft==np.max(fft))[0][0]]      alpha_rando = freq_filter(freq, fft, 2, 20) # used as a normalizing constant and makes     # the detector more invariant to differences across sessions/recordings/individuals          # may need to tune these thresholds to decrease False positives     # 10hz detector     if (freq_filter(freq, fft, 9.9, 10.05) / alpha_rando) &gt; 2:         socketio.emit('my_message', {'data': str(10)+\" hz detected\"})     # 12hz detector     elif (freq_filter(freq, fft, 11.9, 12.2) / alpha_rando) &gt; 2:         socketio.emit('my_message', {'data': str(12)+\" hz detected\"})     # no signal detected     else:         socketio.emit('my_message', {'data': str(int(freq_peak))+\" hz (max freq, no other signal detected)\"})     socketio.sleep() # free up the cpu  In a previous post, we talked about the freq_filter function so all I’ll say on that is that it returns 1 value, which is the magnitude of the signal at the frequency of interest. In the fft function, we take in 1,024 sample values (~4 seconds) from the EEG stream and calculate the componenent frequencies. As described in a previous post, we normalize our signal by background noise (2-20hz) so where you see if (freq_filter(freq, fft, 9.9, 10.05) / alpha_rando) &gt; 2, we are testing if the 10hz signal has been detected by comparing it to our threshold of 2. If the signal has been detected, we want to notify the client (Google Chrome). When the client side receives the signal, a javascript function handles the data and moves the cat in the appropriate direction (10hz was left and 12hz was right). If no signal is detected, we’ll just print out the max frequency that was detected.         Screenshot of the app.   You can see the app in action here: Brain Computer Interface (BCI) using Entrainment with EEG   With all of this in place, we can now explore using machine learning with the EEG signal. One use case to explore is a brain computer interface (BCI). Namely, can we detect a thought pattern that corresponds to a color or shape in the simplest case and a decent size vocabulary in the most complex case? Another use case is mental state recognition. Can we determine how a person is responding to an advertisement or education content in broad brushstrokes? Are they engaged, bored, etc.? A final thought is can multiple sensors give us a complete picture of a person’s mental state? For example, if we fuse EEG with an activity tracker wristband (EDA/GSR, BVP, temperature, and accelerometer) can we spot correlations? These are some of the questions I’ll be exploring next. Stay tuned.  ","categories": ["Biometrics"],
        "tags": ["biometrics"],
        "url": "/biometrics/BCI-with-entrainment/",
        "teaser": "/assets/images/entrainment/flask_app_architecture.png"
      },{
    "title": "About me",
    "excerpt":"      Big Bend National Park, 2022  Hey there! My name is Todd Morrill.   I joined a team of data scientists at my employer PricewaterhouseCoopers (PwC) in 2015 and never looked back. I took 11 math and CS classes through Columbia University and Harvard Extension School while working full-time. I’m now an MS in Computer Science graduate student at Columbia University.   I have led and delivered countless machine learning projects. For example, I have developed document retrieval systems (i.e. search), information extraction systems, and knowledge-graph enabled applications, to name a few. Please see my resume for a more detailed list of projects.   I hope that a masters in CS will serve as preparation to pursue a PhD in CS or for further work in industrial research. I am particularly interested in knowledge representation &amp; reasoning and view it as a critical component to advancing deep learning systems from their current state.         Ziggy in Maine   I currently live in New York City with my girlfriend and our sheepadoodle, Ziggy. Outside of work, I like to hike, camp, ski, and cook. In a former life, I loved to study languages. I still speak Mandarin Chinese well enough.   All views expressed on this site are my own.  ","url": "http://localhost:4000/about/"
  },{
    "title": "Biometrics",
    "excerpt":"Biometrics Posts  ","url": "http://localhost:4000/categories/biometrics/"
  },{
    "title": "Posts by Category",
    "excerpt":" ","url": "http://localhost:4000/categories/"
  },{
    "title": "Favorite Posts",
    "excerpt":"Favorite Posts  ","url": "http://localhost:4000/favorites/"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/"
  },{
    "title": null,
    "excerpt":"var idx = lunr(function () {   this.field('title')   this.field('excerpt')   this.field('categories')   this.field('tags')   this.ref('id')    this.pipeline.remove(lunr.trimmer)    for (var item in store) {     this.add({       title: store[item].title,       excerpt: store[item].excerpt,       categories: store[item].categories,       tags: store[item].tags,       id: item     })   } });  $(document).ready(function() {   $('input#search').on('keyup', function () {     var resultdiv = $('#results');     var query = $(this).val().toLowerCase();     var result =       idx.query(function (q) {         query.split(lunr.tokenizer.separator).forEach(function (term) {           q.term(term, { boost: 100 })           if(query.lastIndexOf(\" \") != query.length-1){             q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })           }           if (term != \"\"){             q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })           }         })       });     resultdiv.empty();     resultdiv.prepend(''+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "http://localhost:4000/assets/js/lunr/lunr-en.js"
  },{
    "title": null,
    "excerpt":"step1list = new Array(); step1list[\"ΦΑΓΙΑ\"] = \"ΦΑ\"; step1list[\"ΦΑΓΙΟΥ\"] = \"ΦΑ\"; step1list[\"ΦΑΓΙΩΝ\"] = \"ΦΑ\"; step1list[\"ΣΚΑΓΙΑ\"] = \"ΣΚΑ\"; step1list[\"ΣΚΑΓΙΟΥ\"] = \"ΣΚΑ\"; step1list[\"ΣΚΑΓΙΩΝ\"] = \"ΣΚΑ\"; step1list[\"ΟΛΟΓΙΟΥ\"] = \"ΟΛΟ\"; step1list[\"ΟΛΟΓΙΑ\"] = \"ΟΛΟ\"; step1list[\"ΟΛΟΓΙΩΝ\"] = \"ΟΛΟ\"; step1list[\"ΣΟΓΙΟΥ\"] = \"ΣΟ\"; step1list[\"ΣΟΓΙΑ\"] = \"ΣΟ\"; step1list[\"ΣΟΓΙΩΝ\"] = \"ΣΟ\"; step1list[\"ΤΑΤΟΓΙΑ\"] = \"ΤΑΤΟ\"; step1list[\"ΤΑΤΟΓΙΟΥ\"] = \"ΤΑΤΟ\"; step1list[\"ΤΑΤΟΓΙΩΝ\"] = \"ΤΑΤΟ\"; step1list[\"ΚΡΕΑΣ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΟΣ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΑ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΩΝ\"] = \"ΚΡΕ\"; step1list[\"ΠΕΡΑΣ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΟΣ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΑ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΩΝ\"] = \"ΠΕΡ\"; step1list[\"ΤΕΡΑΣ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΟΣ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΑ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΩΝ\"] = \"ΤΕΡ\"; step1list[\"ΦΩΣ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΟΣ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΑ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΩΝ\"] = \"ΦΩ\"; step1list[\"ΚΑΘΕΣΤΩΣ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΟΣ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΑ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΩΝ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΓΕΓΟΝΟΣ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΟΣ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΑ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΩΝ\"] = \"ΓΕΓΟΝ\";  v = \"[ΑΕΗΙΟΥΩ]\"; v2 = \"[ΑΕΗΙΟΩ]\"  function stemWord(w) {   var stem;   var suffix;   var firstch;   var origword = w;   test1 = new Boolean(true);    if(w.length '+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "http://localhost:4000/assets/js/lunr/lunr-gr.js"
  },{
    "title": null,
    "excerpt":"var store = [   {%- for c in site.collections -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}     {%- assign docs = c.docs | where_exp:'doc','doc.search != false' -%}     {%- for doc in docs -%}       {%- if doc.header.teaser -%}         {%- capture teaser -%}{{ doc.header.teaser }}{%- endcapture -%}       {%- else -%}         {%- assign teaser = site.teaser -%}       {%- endif -%}       {         \"title\": {{ doc.title | jsonify }},         \"excerpt\":           {%- if site.search_full_content == true -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | jsonify }},           {%- else -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | truncatewords: 50 | jsonify }},           {%- endif -%}         \"categories\": {{ doc.categories | jsonify }},         \"tags\": {{ doc.tags | jsonify }},         \"url\": {{ doc.url | relative_url | jsonify }},         \"teaser\": {{ teaser | relative_url | jsonify }}       }{%- unless forloop.last and l -%},{%- endunless -%}     {%- endfor -%}   {%- endfor -%}{%- if site.lunr.search_within_pages -%},   {%- assign pages = site.pages | where_exp:'doc','doc.search != false' -%}   {%- for doc in pages -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}   {     \"title\": {{ doc.title | jsonify }},     \"excerpt\":         {%- if site.search_full_content == true -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | jsonify }},         {%- else -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | truncatewords: 50 | jsonify }},         {%- endif -%}       \"url\": {{ doc.url | absolute_url | jsonify }}   }{%- unless forloop.last and l -%},{%- endunless -%}   {%- endfor -%} {%- endif -%}] ","url": "http://localhost:4000/assets/js/lunr/lunr-store.js"
  },{
    "title": "Quotes",
    "excerpt":"Plans are useless but planning is indispensable. - Dwight D. Eisenhower  If you’re going to push a piece of machinery to its limits and expect it to hold together, you have to have some sense of where that limit is. - Ken Miles (Ford v Ferrari)  An engineer is said to be a man who knows a great deal about a very little, and who goes around knowing more and more, about less and less, until finally, he knows practically everything about nothing; whereas, a salesman, on the other hand, is a man who knows a very little about a great deal, and keeps knowing less and less about more and more until he knows practically nothing, about everything. - Van Nuys, California, News, June 26, 1933  I believe we have some power over who we love. It isn’t something that just happens to a person. - Amy March (Little Women)  You only win when you maintain your dignity. Dignity always prevails. - Dr. Don Shirley (Green Book)  It is in thy power whenever thou shalt choose to retire into thyself...particularly when he has within him such thoughts that by looking into them he is immediately in perfect tranquility. - Marcus Aurelius (Meditations)  Fortune favors the prepared mind. - Louis Pasteur  The two most important days in your life are the day you are born and the day you find out why. – Mark Twain  quel che sarà, sarà (what will be, will be)  好事多磨 (good things require hard work)","url": "http://localhost:4000/quotes/"
  },{
    "title": "Posts by Tag",
    "excerpt":"","url": "http://localhost:4000/tags/"
  },{
    "title": "Posts by Year",
    "excerpt":"","url": "http://localhost:4000/posts/"
  },{
    "title": null,
    "excerpt":"{% if page.xsl %}{% endif %}Jekyll{{ site.time | date_to_xmlschema }}{{ page.url | absolute_url | xml_escape }}{% assign title = site.title | default: site.name %}{% if page.collection != \"posts\" %}{% assign collection = page.collection | capitalize %}{% assign title = title | append: \" | \" | append: collection %}{% endif %}{% if page.category %}{% assign category = page.category | capitalize %}{% assign title = title | append: \" | \" | append: category %}{% endif %}{% if title %}{{ title | smartify | xml_escape }}{% endif %}{% if site.description %}{{ site.description | xml_escape }}{% endif %}{% if site.author %}{{ site.author.name | default: site.author | xml_escape }}{% if site.author.email %}{{ site.author.email | xml_escape }}{% endif %}{% if site.author.uri %}{{ site.author.uri | xml_escape }}{% endif %}{% endif %}{% if page.tags %}{% assign posts = site.tags[page.tags] %}{% else %}{% assign posts = site[page.collection] %}{% endif %}{% if page.category %}{% assign posts = posts | where: \"category\", page.category %}{% endif %}{% unless site.show_drafts %}{% assign posts = posts | where_exp: \"post\", \"post.draft != true\" %}{% endunless %}{% assign posts = posts | sort: \"date\" | reverse %}{% assign posts_limit = site.feed.posts_limit | default: 10 %}{% for post in posts limit: posts_limit %}{% assign post_title = post.title | smartify | strip_html | normalize_whitespace | xml_escape %}{{ post_title }}{{ post.date | date_to_xmlschema }}{{ post.last_modified_at | default: post.date | date_to_xmlschema }}{{ post.id | absolute_url | xml_escape }}{% assign excerpt_only = post.feed.excerpt_only | default: site.feed.excerpt_only %}{% unless excerpt_only %}{{ post.content | strip | xml_escape }}{% endunless %}{% assign post_author = post.author | default: post.authors[0] | default: site.author %}{% assign post_author = site.data.authors[post_author] | default: post_author %}{% assign post_author_email = post_author.email | default: nil %}{% assign post_author_uri = post_author.uri | default: nil %}{% assign post_author_name = post_author.name | default: post_author %}{{ post_author_name | default: \"\" | xml_escape }}{% if post_author_email %}{{ post_author_email | xml_escape }}{% endif %}{% if post_author_uri %}{{ post_author_uri | xml_escape }}{% endif %}{% if post.category %}{% elsif post.categories %}{% for category in post.categories %}{% endfor %}{% endif %}{% for tag in post.tags %}{% endfor %}{% if post.excerpt and post.excerpt != empty %}{{ post.excerpt | strip_html | normalize_whitespace | xml_escape }}{% endif %}{% assign post_image = post.image.path | default: post.image %}{% if post_image %}{% unless post_image contains \"://\" %}{% assign post_image = post_image | absolute_url %}{% endunless %}{% endif %}{% endfor %}","url": "http://localhost:4000/feed.xml"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/page2/"
  },{
    "title": null,
    "excerpt":" {% if page.xsl %} {% endif %} {% assign collections = site.collections | where_exp:'collection','collection.output != false' %}{% for collection in collections %}{% assign docs = collection.docs | where_exp:'doc','doc.sitemap != false' %}{% for doc in docs %} {{ doc.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if doc.last_modified_at or doc.date %}{{ doc.last_modified_at | default: doc.date | date_to_xmlschema }} {% endif %} {% endfor %}{% endfor %}{% assign pages = site.html_pages | where_exp:'doc','doc.sitemap != false' | where_exp:'doc','doc.url != \"/404.html\"' %}{% for page in pages %} {{ page.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if page.last_modified_at %}{{ page.last_modified_at | date_to_xmlschema }} {% endif %} {% endfor %}{% assign static_files = page.static_files | where_exp:'page','page.sitemap != false' | where_exp:'page','page.name != \"404.html\"' %}{% for file in static_files %} {{ file.path | replace:'/index.html','/' | absolute_url | xml_escape }} {{ file.modified_time | date_to_xmlschema }}  {% endfor %} ","url": "http://localhost:4000/sitemap.xml"
  },{
    "title": null,
    "excerpt":"Sitemap: {{ \"sitemap.xml\" | absolute_url }} ","url": "http://localhost:4000/robots.txt"
  }]
