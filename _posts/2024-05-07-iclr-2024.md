---
title: "ICLR 2024 Roundup"
date: 2024-5-7
categories:
    - machine learning
tags:
    - conferences
    - machine learning
header:
    teaser: /assets/images/iclr_2024.png
excerpt: TLDR; I'm sharing my personal reactions to ICLR 2024.
---
TLDR; I'm sharing my personal reactions to ICLR 2024.

<figure style="width: 540px" class="align-center">
  <img src="/assets/images/iclr_2024.png" alt="ICLR, 2024">
</figure>

I'm an early(ish?) career researcher and I want a pulse on trends in the ML community. Topics that I sought out included neuroscience-inspired AI (e.g., spiking neural networks, local learning rules, etc.), mechanistic interpretability, some ML systems work, and various other topics like continual learning. Here's a hodge-podge of comments, reactions, and takes. Feel free to skip to the [paper summaries below](#day-1---tuesday-may-7-2024).

- I'm paraphrasing *heavily* when I summarize my takeaways from the papers and talks.
- I'm a big fan of the 5-minute presentations that ICLR requested from all authors. Please encourage this at all conferences ! because it makes it so much easier to get a gist of the paper compared to reading the abstract, which is often too high level, or reading the paper, which can often take too long.
- I've short-listed **67** out of **2,296** accepted papers. Of those, I would like to actually read **28** of them. Of those I will probably read ~4.2 papers üòÖ. But even going through this little exercise of giving my one line summary of each of these papers was helpful to get a sense for the design space of paper possibilities AND to see lots of examples of research that made it through the peer-review process.
- Methods matter big time but seeing what problems people are working is really important for me right now. For instance, does learning time delays in spiking neural networks help with task performance? This is an interesting problem and if you sit down to think about it, you might come up with a dozen methods for approaching the problem. The problem is the interesting thing in this example.
- It's possible that GPT-5 will solve all of our problems but if not, then people may start looking for the "next thing". SNNs and neuromorphic chips may be the next battleground in AI in terms of both algorithms and hardware. I think there were enough papers on these topics at ICLR to say this is a trend.

‚≠êÔ∏ès indicate papers that I'm particularly interested in reading.

# Day 1 - Tuesday, May 7, 2024
1. [**Predictive auxiliary objectives in deep RL mimic learning in the brain**](https://openreview.net/forum?id=agPpmEgf8C) - Auxuliary predictive objectives improve RL learning. My sense is that in addition to learning from reward signals, they introduce self-supervised learning objectives (e.g., next-token prediction, etc.) to improve learning. Examining the learned representations, they show that they resemble representations in the brain.
    <details>
        <summary>Extra notes</summary>
        - Cited previous work like Dan Yamins, and Jim DiCarlo showing representational similarity between CNNs and visual cortex.<br>
        - Authors claim deep RL can be a useful framework for thinking abount interacting brain regions (e.g., hippocampus). In particular they map the predictive network to the hippocampus and the q-learning module to the striatum (what's the <a href="https://en.wikipedia.org/wiki/Striatum">striatum</a>?).<br>
        - Predictive objectives include next-state prediction and maximizing the distance between randomly sampled different states.
    </details>

1. [**Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs**](https://openreview.net/forum?id=uNrFpDPMyo) - This work analyzes attention patterns in transformer based language models to determine which tokens are being attended to. This can be used to determine which tokens can be discarded from the key-value cache, which is a major bottleneck for LLM inference on GPUs since they are memory bandwidth constrained. FlashAttention is another solution to the same problem - limited GPU memory bandwidth.

1. ‚≠êÔ∏è [**A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks**](https://openreview.net/forum?id=RyUvzda8GH) - This work iterates on the idea of predictive coding. Predictive coding is the idea that neural representations (in the brain and in ANNs?) should be suited for predicting the next state from the current state. I see this as very similar to most self-supervised learning objectives.
    <details>
        <summary>Extra notes</summary>
        - Authors point to the potential biological implausibility of backpropagation in the brain.<br>
        - They call for parallelization, locality, and automation in learning algorithms for non-von-Neumann architectures.<br>
        - In computational models, this is done via a minimization of the variational free energy, in this case a function of the total error of the generative model. <b>TODO:</b> read up on variational free energy.<br>
    </details>
    <!-- **TODO:** read this paper -->

1. [**BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity**](https://openreview.net/forum?id=mQYHXUUTkU) - The authors present a method for assigning semantically meaningful labels to functional areas of the visual cortex (e.g., this little voxel activates for faces, this one for houses, etc.). Best I can tell, they use a model that predicts from image space to voxel-wise brain activations and they search for the image that maximally activates a voxel. Then they use a vision-language multi-modal model to generate a text caption for the image that maximally activates the voxel.

1. [**Circuit Component Reuse Across Tasks in Transformer Language Models**](https://openreview.net/forum?id=fpoAYV6Wsk) - This mechanistic interpretability paper provides evidence for the hypothesis that circuit components found in small transformers (e.g., GPT-2) may also be the same components used in larger models (e.g., GPT-2 Large). This would be desirable as it allows us to move towards a more general taxonomy of transformer components that are not specific to a single model and task.

1. [**Conformal Risk Control**](https://openreview.net/forum?id=33XGfHLtZg) - The authors extend conformal prediction to control the expected value of any monotone loss function. Recall conformal prediction adjusts the size of the prediction set to guarantee coverage (e.g., expand regression prediction interval to guarantee the correct answer is in the set 90% of the time). The generalization here is to just consider other loss functions such as the false negative rate. If the loss function is the miscoverage loss, then their proposed risk control is equivalent to conformal prediction.

1. [**Initializing Models with Larger Ones**](https://openreview.net/forum?id=dyrGMhicMw) - In this work, the authors propose to initialize neural network model weights for a smaller model using weights from larger models. This reminds me of two related ideas: knowledge distillation and Tony Zador's genomic bottleneck idea. <!-- **TODO:** Share -->

1. [**Leveraging Generative Models for Unsupervised Alignment of Neural Time Series Data**](https://openreview.net/forum?id=9zhHVyLY4K) - My TLDR; this paper tries to use ideas of pretraining used for foundation models for models that learn patterns in neural data across recording sessions, tasks, and animals in contrast to the traditional approach, which is to build a generative model for each recording session, task or animal.

1. ‚≠êÔ∏è [**Masks, Signs, And Learning Rate Rewinding**](https://openreview.net/forum?id=qODvxQ8TXW) - Not super familiar with learning rate rewinding (LRR) as a method for finding lottery ticket sparse subnetworks but they say that LRR is effective for finding sparse subnetworks and are robust to sign changes in the parameters learned in the model. ü§∑‚Äç‚ôÇÔ∏è <!-- **TODO:** read this paper -->

1. ‚≠êÔ∏è [**Sparse Autoencoders Find Highly Interpretable Features in Language Models**](https://openreview.net/forum?id=F76bwRSLeK) - This is the type of work that I personally find very interesting. They learn a sparse codebook of interpretable features found in transformer models using sparse autoencoders to help with the challenge of polysemanticity of neurons. This type of decomposition can be useful for mechanistic explanations of the algorithms that deep learning models are implementing. <!-- **TODO:** read this paper -->

1. [**Synaptic Weight Distributions Depend on the Geometry of Plasticity**](https://openreview.net/forum?id=x5txICnnjC) - This work suggests that the Euclidean distance assumed for e.g., backpropagation results in synaptic weights that follow a different distribution from those found in the human brain, implying that other distance measures (and therefore geometry) may be more appropriate for synaptic weight learning.

1. [**Towards Best Practices of Activation Patching in Language Models: Metrics and Methods**](https://openreview.net/forum?id=Hf17y6u9BC) - This is definitely a paper of interest for me. They examine two types of activation patching used for circuit discovery in mechanistic interpretability and show that counterfactual prompting appears to work best compared to adding Gaussian noise to input tokens.

1. [**Vision Transformers Need Registers**](https://openreview.net/forum?id=2dnO3LLiJ1) - The authors add filler tokens to vision transformers that don't correspond to any real input to serve as registers. This results in interpretable vision maps. Follow-up reading: Massive Activations in Large Language Models.
    <details>
        <summary>Extra notes</summary>
        - Use [CLS] attention map in vision transformers to understand what the model is attending to.<br>
        - There's a real variety of attention patterns in vision transformers - many that seem spurious.<br>
        - High-norm tokens are aggregators that are collecting information from similar tokens.<br>
        - Register tokens take on object oriented attention behavior.
    </details>

1. [**The mechanistic basis of data dependence and abrupt learning in an in-context classification task**](https://openreview.net/forum?id=aN4Jf6Cx69) - They describe in-context learning abilities in terms of attention maps, the input context, and induction heads.
    <details>
        <summary>Extra notes</summary>
        - Explained their results in terms of induction heads<br>
        - Talked about the data that leads to in-context learning or memorization.<br>
        - Distilled the induction head to two parameters that explain the in-context learning skills.
    </details>

1. [**Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection**](https://openreview.net/forum?id=hSyW5go0v8) - Their framework trains a LM that adaptively retrieves passages by predicting a special token that tells the model to do retrieval. They then feed the retrieved content back into the model with the user query, score the generations, and generate more special tokens to indicate which of the responses is best. It was a bit of a frankenstein system.

1. ‚≠êÔ∏è [**A differentiable brain simulator bridging brain simulation and brain-inspired computing**](https://openreview.net/forum?id=AU2gS9ut61) - This paper introduces a Python framework BrainPy, which attempts to implement both differentiable spiking neural networks alongside traditional brain simulation models (e.g., Hodgkin-Huxley). It was developed using JAX and XLA. <!-- **TODO:** read this paper -->

1. [**BrainLM: A foundation model for brain activity recordings**](https://openreview.net/forum?id=RwI7ZEfR27) - This paper essentially does large scale language model style pre-training using a huge collection of fMRI recordings and shows that the resulting representations are quite good for downstream *static* classification tasks like age, anxiety, and PTSD. It's less clear if the learned representations are useful for decoding brain activity for *dynamic* tasks like inferring what's in the image someone is looking at.

1. [**Critical Learning Periods Emerge Even in Deep Linear Networks**](https://openreview.net/forum?id=Aq35gl2c1k) - Critical learning periods are time periods early in development where temporary sensory deficits can permanently damage the outcome of learning. This paper focuses on an analytical model of deep *linear* models (ignoring the complexity of non-linearities, etc.) and shows that both the depth of the neural network architecture and the data distributions under study can lead to critical learning periods. They liken this to classic Hubel and Wiesel experiments showing that early sensory deprivation can lead to permanent deficits in visual processing.

1. ‚≠êÔ∏è [**Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data**](https://openreview.net/forum?id=aGH43rjoe4) - This paper attempts to use a VAE to learn latent variable representations of neural and behavioral data jointly. It attempts to disentangle which dimensions are representing which modality by attempting to reconstruct the data from a subset of the latent variables. <!-- **TODO:** read this paper -->

1. [**Scaling Laws for Associative Memories**](https://openreview.net/forum?id=Tzh6xAJSll) - The authors define a mathematical abstraction of a transformer model using high-dimensional outer products and characterize the memory capacity of the model as the parameter count grows.


# Day 2 - Wednesday, May 8, 2024
1. [**ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models**](https://openreview.net/forum?id=osoWxY8q2E) - This is mostly an ML systems paper that takes advantage of the simple computation of ReLU as compared to more complicated activation functions like GELU. They exploit the sparsity of ReLU activations to speed up LLM inference by observing that if an output of a neuron is zero, then the corresponding incoming weights can be ignored. It's just not obvious to me how they determine this ahead of time!

1. ‚≠êÔ∏è [**Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks**](https://openreview.net/forum?id=xwKt6bUkXj) - This paper examines recurrent neural network abilities to perform a task that requires increasing memory over time. The task is $N$-parity, which looks at the trailing $N$ inputs and has to compute the parity of those inputs. They study the both the timescale of individual neurons (think time constant $\tau$ corresponding to biophysical properties of spike passing) and the network mediated timescale, which they say is the rate at which neurons in a network decorrelate their spiking activity. <!-- **TODO:** read this paper -->

1. ‚≠êÔ∏è [**Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization**](https://openreview.net/forum?id=My7lkRNnL9) - Very cool work in the spirit of local learning rules and alternatives to backpropagation. They provide some unifying principles that link a number of forward-only learning procedures. <!-- **TODO:** read this paper -->

1. ‚≠êÔ∏è [**Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching**](https://openreview.net/forum?id=Ebt7JgMHv1) - This paper explores the problem of trying to determine if activation patching is truly discovering the underlying circuits in the model. <!-- **TODO:** read this paper -->

1. [**Local Search GFlowNets**](https://openreview.net/forum?id=6cFcw1Rxww) - I know very little about GFlowNets and need to do some more reading. But Yoshua Bengio is bullish on them so I'm interested.

1. ‚≠êÔ∏è [**Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization**](https://openreview.net/forum?id=SNGXbZtK6Q) - Studying the out-of-distribution problem from a neuron activation perspective seems like a natural thing to want to do (versus e.g., the input space or the entropy of the model's predictions). <!-- **TODO:** read this paper -->

1. [**Prompt Gradient Projection for Continual Learning**](https://openreview.net/forum?id=EH2O3h7sBI) - This paper mashes up two existing ideas: 1) if you project gradients in the orthogonal direction to existing gradients, you can avoid catastrophic forgetting and 2) prompt-tuning, which introduces new trainable tokens to the model to help it learn new tasks.

1. ‚≠êÔ∏è [**Scaling Laws for Sparsely-Connected Foundation Models**](https://openreview.net/forum?id=i9K2ZWkYIP) - This paper introduces scaling laws for transformers as the sparsity level (the number of non-zero parameters) and the dataset size vary. <!-- **TODO:** read this paper -->

1. [**The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks**](https://openreview.net/forum?id=vE1e1mLJ0U) - The abstract alone on this paper just makes you say wow. On a first read it looks like individual neurons in the brain are so incredibly complex that it requires neural networks with tens of thousands of parameters to model them.

1. **Mechanistic Interpretability Social Meetup**
    - Packed! The space is very high growth right now
    - Behavioral tests insufficient because models might have, "I'm being evaluated circuits."
    - "How to avoid getting scooped?" - will have to read a lot and find time to do your own thing
    - Many good 1-on-1 discussions after the panel finished

1. ‚≠êÔ∏è [**Bayesian Bi-clustering of Neural Spiking Activity with Latent Structures**](https://openreview.net/forum?id=ZYm1Ql6udy) - The author proposes a Bayesian procedure for identifying clusters of neural activity in both time and space. <!-- **TODO:** read this paper -->

1. ‚≠êÔ∏è [**From Sparse to Soft Mixtures of Experts**](https://openreview.net/forum?id=jxpsAj7ltE) - Mixture of experts in language models is a way to route tokens to specific parameters in the model so you can avoid having to use all model parameters. Not only is this ideally more compute efficient (this is dynamic computation - closer to what the brain does) but maybe it allows for more specialization among the experts. You can do hard assignments or soft assignments (e.g., send some portion of the token to each expert). I think in this work, they're putting a distribution over tokens and sending a little bit of each token to all experts. So a token that gets sent to an expert is a weighted combination of *all* tokens. <!-- **TODO:** read this paper -->

1. [**How connectivity structure shapes rich and lazy learning in neural circuits**](https://openreview.net/forum?id=slSmYGc8ee) - This work builds on the idea that there are different learning regimes for neural networks: 1) rich learning where the network modifies its network weights quite a bit, and 2) lazy learning where the network doesn't modify its weights much. Rich learning may be better at generalization by learning to ignore irrelevant features, for example. In this work, they look at how different initialization strategies (e.g., random or connectivity structures mimicking biological networks) affect the learning regime of the network. TLDR; the weights change much more when the network is initialized with a biological connectivity structure compared with random initialization. The learned parameters are also low-rank. Low-initial rank also seems to lead to more rich learning (as measured by the frobenius norm of the weight updates). The exception may be when evolutionary/inductive biases may lead to good initial performance. <!-- **TODO:** Share -->

1. [**Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI**](https://openreview.net/forum?id=QzTpTRVtrP) - This paper is very similar in spirit to the BrainLM paper above, where instead of fMRI data, they're working with EEG data.

1. ‚≠êÔ∏è [**Learning dynamic representations of the functional connectome in neurobiological networks**](https://openreview.net/forum?id=ZwhHSOHMTM) - This work attempts to identify which neural circuitry is working together within a short period of time, while accounting for the fact that these circuits may change over time. <!-- **TODO:** read this paper -->

1. ‚≠êÔ∏è [**Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks**](https://openreview.net/forum?id=A0HKeKl4Nl) - The authors of this work showed an interesting result, which was that fine-tuning doesn't change the internal representations of language models all that much, but rather it learns a wrapper around existing capabilities, where you might simply pre or post-process the input or output to the internal capability of the model to get the desired behavior. <!-- **TODO:** read this paper -->

1. ‚≠êÔ∏è [**Pre-Training and Fine-Tuning Generative Flow Networks**](https://openreview.net/forum?id=ylhiMfpqkm) - This work attempts to define an unsupervised pre-training objective for GFlowNets, which typically require a reward function to be specified. <!-- **TODO:** read this paper -->

1. ‚≠êÔ∏è [**TAB: Temporal Accumulated Batch Normalization in Spiking Neural Networks**](https://openreview.net/forum?id=k1wlmtPGLq) - Batch normalization is a technique used to stabilize training in deep neural networks by normalizing the activations of a layer over the batch dimension. It works by reducing internal covariate shift, which happens when the distribution of the activations changes after weight updates, resulting in confused downstream layers. In SNNs, BatchNorm becomes harder for a few reasons: 1) the activations are spikes, not continuous values, and 2) there's a temporal dimension to the activations. This work introduces a new normalization technique that takes into account the temporal dimension of the activations. Best I can tell they're computing a moving average of mean and variance statistics over the time-dimension before applying the normalization. <!-- **TODO:** read this paper -->

1. ‚≠êÔ∏è [**What does the Knowledge Neuron Thesis Have to do with Knowledge?**](https://openreview.net/forum?id=2HJRwwbV3G) - This paper makes the claim that factual information isn't simply stored in the MLP neurons of a language model as the paper "Locating and editing factual associations in GPT." from 2022 seems to imply. <!-- **TODO:** read this paper -->

# Day 3 - Thursday, May 9, 2024
1. ‚≠êÔ∏è [**Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps**](https://openreview.net/forum?id=mYWsyTuiRp) - MLPs are hard to understand from a mechanistic interpretability perspective, which I think is the case due to not being able to linearly decompose activations because of the non-linear activation functions. I'm having a hard time getting a gist for this paper - probably need to read it in its entirety. Maybe they're trying to measure how much outputs from MLPs affect downstream attention calculations? <!-- **TODO:** read this paper -->

1. ‚≠êÔ∏è [**Dictionary Contrastive Learning for Efficient Local Supervision without Auxiliary Networks**](https://openreview.net/forum?id=Gg7cXo3S8l) - In this work, the authors are trying to train models using only local supervision (e.g., no backprop) with a contrastive learning based approach to pushes together activations from the same class and pushes apart different classes. Their innovation was to add a bank of learned class representations that are used to compute the contrastive loss (i.e., an instance of the class should be similar to the class representation). <!-- **TODO:** maybe read this paper? Mostly interested in their literature review to get a pulse on this topic. -->

1. [**In-Context Pretraining: Language Modeling Beyond Document Boundaries**](https://openreview.net/forum?id=LXVswInHOo) - The idea of this work builds on a quirk of modern LLM training, which packs random documents together to form a single training instance. This is for efficiency, but the downside is that later documents can attend back to completely unrelated documents. This decision has always perplexed me. In this work, they find approximately related documents to pack into a single training instance and show it helps with downstream tasks.

1. ‚≠êÔ∏è [**Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings**](https://openreview.net/forum?id=4r2ybzJnmN) - Theoretical work has shown that spiking neural networks that have adjustable time-delays are more expressive than those that don't. This work introduces a new method for learning time-delays in SNNs. <!-- **TODO:** read this paper -->

1. ‚≠êÔ∏è [**Online Stabilization of Spiking Neural Networks**](https://openreview.net/forum?id=CIj1CVbkpr) - This paper appears to be similar to the paper above, TAB, which attempted to implement BatchNorm for SNNs. <!-- **TODO:** read this paper -->

1. [**Pre-training with Random Orthogonal Projection Image Modeling**](https://openreview.net/forum?id=z4Hcegjzph) - Vision transformers can be trained through a masked modeling objective where patches are dropped from the image and the model attempts to predict the missing bits. This work takes a different approach by randomly projecting the image patches and attempting recover the original image patch. This might lead to a stronger learning signal and a smoother signal at that. They use sketching methods to implement the random projections. It's slightly reminiscent of diffusion models because they're predicting a sort of additive noise to the image. They show models train faster with their technique.

1. ‚≠êÔ∏è [**Spatio-Temporal Approximation: A Training-Free SNN Conversion for Transformers**](https://openreview.net/forum?id=XrunSYwoLr) - This paper provides a method for converting a pretrained transformer model to a spiking neural network. The challenge is that operations like self-attention and normalization pose challenges for SNNs, which have to compute through time. <!-- **TODO:** read this paper -->

1. ‚≠êÔ∏è [**Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips**](https://openreview.net/forum?id=1SIBN5Xyw7) - This paper focuses on transformer based SNNs (in contrast to CNN based SNNs) and how they might inspire next-generation neuromorphic chips. <!-- **TODO:** read this paper -->

1. [**Towards Understanding Factual Knowledge of Large Language Models**](https://openreview.net/forum?id=9OevMUdods) - They introduce a dataset of fact related questions to probe LLM factuality.

1. ‚≠êÔ∏è [**ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis**](https://openreview.net/forum?id=oTRwljRgiv) - They probe LLM abilities to piece together program subroutines into coherent programs. At a first glance, they might be using LLM agents to solve these compositional tasks so it could be cool to see another example of using LLM agents. <!-- **TODO:** take a look at the datasets they used in this paper. -->

1. ‚≠êÔ∏è [**A Framework for Inference Inspired by Human Memory Mechanisms**](https://openreview.net/forum?id=vBo7544jZx) - This paper uses the (potentially old?) idea of trying to model short and long-term memory systems. I think moving toward a multi-tiered memory system seems like a fun and potentially fruitful research direction for AI systems. <!-- **TODO:** read this paper -->

1. ‚≠êÔ∏è [**A Progressive Training Framework for Spiking Neural Networks with Learnable Multi-hierarchical Model**](https://openreview.net/forum?id=g52tgL8jy6) - The authors of this work roughly say the Leaky Integrate-and-Fire (LIF) neuron underperforms with deep-layer gradient calculation and capturing global information on the time dimension. They propose a new neuron to alleviate these issues. To draw a probably imperfect analogy, this feels like going from a vanilla RNN cell to an LSTM cell. <!-- **TODO:** read this paper -->

1. [**Complex priors and flexible inference in recurrent circuits with dendritic nonlinearities**](https://openreview.net/forum?id=S5aUhpuyap) - The authors are interested in the question of how the brain represents priors about the world (e.g., the structure of faces, etc.). They put a probability distribution over neural circuits. They also draw inspiration from diffusion models and liken the prior to a data manifold and claim that diffusion modeling is like trying to take steps to get back onto the data manifold.

1. [**Implicit regularization of deep residual networks towards neural ODEs**](https://openreview.net/forum?id=AbXGwqb5Ht) - This work investigates links between discrete (e.g., ResNets) and continuous time neural ODEs. The key result seems to be that if the network is initialized as a discretization of a neural ODE (not sure what this means), then such a discretization holds throughout training. I would have to look more closely at the paper to unpack this.

1. [**Manipulating dropout reveals an optimal balance of efficiency and robustness in biological and machine visual systems**](https://openreview.net/forum?id=ADDCErFzev) - This work examines what happens when you crank up the dropout level and 1) evaluate the model's efficiency (I think measured by the dimensionality of information where higher-dimensions carries more information?), 2) evaluate robustness of the representations (robustness to data perturbations?), and 3) compare representational similarity between the model and the brain.

1. [**Modeling state-dependent communication between brain regions with switching nonlinear dynamical systems**](https://openreview.net/forum?id=WQwV7Y8qwa) - This work attempts to provide a descriptive model of how brain regions communicate with each other.

1. [**One-hot Generalized Linear Model for Switching Brain State Discovery**](https://openreview.net/forum?id=MREQ0k6qvD) - This is a Bayesian treatment of identifying functional connections between disparate brain regions as the functional connectivity changes over time (e.g., when the subject is performing different tasks).

1. [**Parsing neural dynamics with infinite recurrent switching linear dynamical systems**](https://openreview.net/forum?id=YIls9HEa52) - This is probably a very crude approximation of what this paper is about but it seems to be investigating low-dimensional dynamics of neural activity while allowing those dynamics to vary as the brain "changes states" such as when the subject is performing different tasks.

1. [**Towards Energy Efficient Spiking Neural Networks: An Unstructured Pruning Framework**](https://openreview.net/forum?id=eoSeaK4QJo) - This is a systems paper for SNNs that attempts to sparsify networks with an eye towards energy savings on neuromorphic chips.

# Day 4 - Friday, May 10, 2024
1. [**Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data**](https://openreview.net/forum?id=W8S8SxS9Ng) - This paper attempts to pre-train general purpose transformers on multi-modal brain data (e.g., neural recordings, behavior data, etc.) and show that the representations learned are useful for downstream tasks. Clearly there was a trend of trying to do unsupervised pre-training of transformers on brain data at this conference.

1. ‚≠êÔ∏è [**SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition**](https://openreview.net/forum?id=7etoNfU9uF) - This paper proposes an SNN for event detection in streams of camera data. <!-- **TODO:** read this paper -->

1. **Sasha Rush Session** - There were career chat sessions with tons of big-name researchers. Sasha answered all sorts of questions about career advice and where the field of NLP is heading. This was a cool idea by the conference organizers to host these sessions.

1. [**Provable Compositional Generalization for Object-Centric Learning**](https://openreview.net/forum?id=7VPTUWkiDQ) - This was a bit of a theory paper that defined what it means for a model (an autoencoder) to generalize to novel compositions of objects. Compositionality is appealing because it allows us to generalize to novel objects by combining known objects in novel ways. For instance, if we know what legs of a chair look like and we know what tops of tables look like, we might be able to imagine what a table with legs might look like.

1. [**BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models**](https://openreview.net/forum?id=3TO3TtnOFl) - This is a retrieval augmented generation (RAG) paper that speeds up the retrieval process by using binary token representations. Question: is it common to pass both the query and document to be retrieved through a transformer model? I thought sentence embeddings were used for retrieval.

1. [**Decoding Natural Images from EEG for Object Recognition**](https://openreview.net/forum?id=dhLIno8FmH) - This works follows a long line of work trying to predict what someone is thinking about (e.g., an image label, etc.) based on their brain activity. This paper uses two encoders - an image encoder and an EEG encoder - with a constrastive loss to learn a shared representation between the two modalities. They also introduce some new spatial attention mechanisms for the EEG encoder.

1. [**Overthinking the Truth: Understanding how Language Models Process False Demonstrations**](https://openreview.net/forum?id=Tigr1kMDZy) - This mechanistic interpretability paper sets up careful counterfactual prompts. The correct prompt shows $k$ text examples with their corresponding correct sentiment labels and the counterfactual is to flip the labels to be incorrect. They ablate attention heads by zeroing out their outputs to observe which has the most impact on the model's in-context learning of the incorrect labels. TLDR; models might use separate circuits for prompts that reflect reality/truth and those that are more adversarial or are incorrect.

1. [**Epitopological learning and Cannistraci-Hebb network shape intelligence brain-inspired theory for ultra-sparse advantage in deep learning**](https://openreview.net/forum?id=iayEcORsGd) - My gist (which may not be precise) after talking with the authors is that they have a network science method to initialize sparse connectivity structures in neural networks, which serve as good starting points for gradient descent. <!-- **TODO:** Share -->

1. [**Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks**](https://openreview.net/forum?id=MeB86edZ1P) - It's very funny how the title of this paper contains all my favorite buzzwords. They build on the classic idea from continual learning to only update weights using gradient directions that are orthogonal to the directions of previous tasks.

1. [**Successor Heads: Recurring, Interpretable Attention Heads In The Wild**](https://openreview.net/forum?id=kvcbV8KQsi) - This paper examines the mechanisms responsible for incrementation tasks (e.g., today is Tuesday so tomorrow is Wednesday) in transformers.

1. ‚≠êÔ∏è [**Traveling Waves Encode The Recent Past and Enhance Sequence Learning**](https://openreview.net/forum?id=p4S5Z6Sah4) - This work treats encoded information as waves and shows how different information can be combined and decoded using these waves. It's an interesting idea for short-term memory and I think it has analogs to recent state space models. <!-- **TODO:** read this paper -->

# Day 5 - Saturday, May 11, 2024
1. [**First Workshop on Representational Alignment (Re-Align)**](https://representational-alignment.github.io/) - This workshop was aimed to examining representational alignment between primarily neural networks and neural data. In particular, their stated aim was defining, evaluating, and understanding the implications of representational alignment among biological & artificial systems. The organizers released a position/survey paper that you might find interesting - [https://arxiv.org/abs/2310.13018](https://arxiv.org/abs/2310.13018). There were lots of interesting talks and posters.