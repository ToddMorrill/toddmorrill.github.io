---
layout: post
title: "KMeans Clustering Results"
date: 2017-07-22
---

Last time we left off with our motivation for working EEG data given that machine learning and deep learning, specifically, might do a better job identifying patterns in the EEG data. As a first exercise, I would like to accomplish two things: 1) do some exploratory data analysis on EEG data to better understand this data format, and 2) determine what features can be created and what they might be useful for. KMeans is a great algorithm for separating data into clusters when you don't have labels. There are a number of assumptions to look out for and cluster evaluation is more of an art than a science but I'll try to provide some color on this as we go through the results.

<div style="text-align:center;"><img src="/assets/kmeans/raw_16_channels.png"></div>
*Raw 16 channel EEG data, which shows relative microvolts over time (relative to bias electrode)*

The data preprocessing steps can be overwhelming when first starting out with EEG data so let's take it one step at a time and look at some code along the way.

``` python
import numpy as np
```

```python
import urllib2
[...]
```

{% highlight python %}
import urllib2
[...]
{% endhighlight %}

If you're interested in learning more about the setup and EEG signals more broadly, I'd recommend <a style="color:#0000FF" href="eeghacker.blogspot.com" target="_blank">Chip Audette's</a> blog. I intend to pick up where this blog leaves off. Two other companies doing neat stuff with biometrics data are <a style="color:#0000FF" href="imotions.com" target="_blank">imotions.com</a> and <a style="color:#0000FF" href="biopac.com" target="_blank">biopac.com</a>.

This week I’m going to introduce the hardware we’ve chosen:

- <a style="color:#0000FF" href="https://shop.openbci.com/collections/frontpage/products/ultracortex-mark-iv?variant=23280741955" target="_blank">16 Channel OpenBCI Mark 4 Headset</a>
- <a style="color:#0000FF" href="https://www.empatica.com/e4-wristband" target="_blank">Empatica E4 Wristband</a>

UPS lost the E4 Wristband so I’m still waiting for that to arrive, but the OpenBCI Mark 4 Headset arrived in the mail and I want to recap what I learned while assembling it.

The OpenBCI Mark 4 Headset comes in a few different forms: assembled, unassembled, or self 3D printed. I chose unassembled and put the headset together myself. Here were a few things that I saw along the way.

<div style="text-align:center;"><img src="/assets/assembly_1.JPG"></div>

The documentation page was super helpful for assembling the headset. I didn’t have all the tools they listed by I did just fine assembling the headset with a razor blade, some sandpaper, a screw driver, and super glue. The docs make it seem like you only need one super glue but I ended up needing 2 .14oz Loctite super glues. I’d also recommend gloves as you need to apply a lot of glue to this headset and you’re bound to get a bunch on your fingers. Have fun picking that off for the next couple hours.. I found out the hard way.

<div style="text-align:center;"><img src="/assets/assembly_2.JPG"></div>

After painstakingly glueing everything together, you arrive at the electronics. It’s fairly obvious how the daisy board (the smaller board) sits on top of the main board (the bigger board) and how the USB Bluetooth receiver works. The trickier part, however, is pinning down all those wires running from the electrodes back to the cyton boards; in my case, 16 channels. It was also unclear why I was getting a “RAILED” error message for the 9-16 channels from the OpenBCI GUI when I plugged everything in and tried to stream EEG waves. It turns out that you need to use the splitter wire provided in the Cyton board box to connect the “SRG ground” signal to both the main board and the daisy board. After that fix, I was up and running.

I promptly recorded about a minute of <a style="color:#0000FF" href="https://github.com/ToddMorrill/EEG/tree/master/kmeans/data" target="_blank">16 channel + accelerometer data</a> if anyone is interested in seeing that. I’ve attached a sample data set below where I start normal (eyes open), then close my eyes (should see alpha waves), clench my jaw (lots of artifacts), and then shake my head side-to-side and front-to-back (see what the accelerometer picked up). My hypothesis is that kmeans might be able discriminate between the ~5 different actions above.

<div style="text-align:center;"><img src="/assets/assembly_4.JPG"></div>

Next up, getting into the code.