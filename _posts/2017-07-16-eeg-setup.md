---
layout: post
title: "EEG Setup"
date: 2017-07-16
---

<p>tup and EEG signals more broadly, I'd recommend Chip Audette's eeghacker.blogspot.com blog. I intend to pick up where this blog leaves off. Two other companies doing neat stuff with biometrics data are imotions.com and biopac.com.

This week I’m going to introduce the hardware we’ve chosen:
16 Channel OpenBCI Mark 4 Headset - https://shop.openbci.com/collections/frontpage/products/ultracortex-mark-iv?variant=23280741955
Empatica E4 Wristband - https://www.empatica.com/e4-wristband

UPS lost the E4 Wristband so I’m still waiting for that to arrive, but the OpenBCI Mark 4 Headset arrived in the mail and I want to recap what I learned while assembling it.

The OpenBCI Mark 4 Headset comes in a few different forms: assembled, unassembled, or self 3D printed. I chose unassembled and put the headset together myself. Here were a few things that I saw along the way.

The documentation page was super helpful for assembling the headset. I didn’t have all the tools they listed by I did just fine assembling the headset with a razor blade, some sandpaper, a screw driver, and super glue. The docs make it seem like you only need one super glue but I ended up needing 2 .14oz Loctite super glues. I’d also recommend gloves as you need to apply a lot of glue to this headset and you’re bound to get a bunch on your fingers. Have fun picking that off for the next couple hours.. I found out the hard way.

After painstakingly glueing everything together, you arrive at the electronics. It’s fairly obvious how the daisy board (the smaller board) sits on top of the main board (the bigger board) and how the USB Bluetooth receiver works. The trickier part, however, is pinning down all those wires running from the electrodes back to the cyton boards; in my case, 16 channels. It was also unclear why I was getting a “RAILED” error message for the 9-16 channels from the OpenBCI GUI when I plugged everything in and tried to stream EEG waves. It turns out that you need to use the splitter wire provided in the Cyton board box to connect the “SRG ground” signal to both the main board and the daisy board. After that fix, I was up and running. I promptly recorded about a minute of 16 channel + accelerometer data if anyone is interested in seeing that. I’ve attached a sample data set below where I start normal (eyes open), then close my eyes (should see alpha waves), clench my jaw (lots of artifacts), and then shake my head side-to-side and front-to-back (see what the accelerometer picked up). My hypothesis is that kmeans might be able discriminate between the ~5 different actions above.

Next up: Getting into the code
</p>
