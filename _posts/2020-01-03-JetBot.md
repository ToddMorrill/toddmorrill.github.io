---
layout: post
title: "Building a JetBot"
date: 2020-06-03
categories:
    - Robotics & Hardware
commentIssueId: 37
---
TLDR; I built a little JetBot equipped with a deep learning model based collision avoidance system.

<br>
<div style="text-align:center;"><a href="https://www.youtube.com/watch?v=iqVKjye-J68" target="_blank"><img src="/assets/jetbot/youtube.png" style="max-width:720px"></a></div>
<div style="text-align:center">Check out the 1 minute <a href="https://www.youtube.com/watch?v=iqVKjye-J68" target="_blank">YouTube video</a></div>
<br>

A couple years ago, Nvidia released a smallish piece of hardware called the Jetson Nano[link], which is essentially a Raspberry Pi with a GPU onboard. At about the same time, Nvidia open-sourced a build specification for a little robot that they dubbed, the JetBot[link]. At the time, one had to methodically go through the bill of materials[link] and round up all the parts to build the thing from a number of different vendors (now they're easy to come by[link]). So indeed that's what I did. And the parts sat in there in their little bubble wrapped Amazon bags and Adafruit boxes for the better part of a year, reminding me that I *really* should do something with them, if I ever got the time. Sadly other projects took precendence and more importantly, I was missing two essential ingredients: a 3D printed chassis for the bot and a soldering iron.

Fast forward to early March, 2020 when I was still going into the office to work with my colleagues, a narrow window of opportunity presented itself between other projects. I emailed my colleagues in Chicago for help with the 3D printing and soldering on a Sunday night, flew to Chicago with the goods Monday morning (as our team often does when COVID isn't dominating the world), and had the bot assembled and driving around by Tuesday night. What a fun little hackathon with my teammates!

Sara, who you will see showing my the 3D design files in the YouTube video[link], is our resident 3D printing expert. She was thrilled to help out because we had just gotten a new toy at work - a MarkForged Mark Two 3D Printer, fully equipped with a carbon fiber print head that prints composite parts as strong as aluminum. This was Sara's maiden voyage with the Mark Two, and *everyone* was impressed with the smooth finish of the print job and strength of the result.

Next up was Sanika, who graciously helped me solder the electronics together, namely the wires to connect the LCD screen that displays the IP address of the bot so that you can connect to it wirelessly. Sanika patiently showed me how to solder. I gave it a shot and maybe had one successful joint for every 5 that I attempted. Thankfully, she came back through and cleaned up my work.

At this point, all that was left was assembly, which took a couple solid hours of quiet and attentive focus. You can see the light slowly fade outside and the sustenance (read popcorn, take-out, and cookies) appear in the video clip. The final moments of assembly were a little harrowing. By that point, I sorely needed a break and made the one mistake that was majorly warned against in the instructions by crossing some wires. Luckily no damage was done and boy was I a happy camper when I booted the bot up and it responded to my controls.

Then life happened again and I put the bot down for a couple week, at which point I got motivated to at least collect the data needed for the collision avoidance system. This is an autonomous bot after all, so it needs some way of avoiding objects that it's about to collide with. I followed the instructions and took about 250 photos good/bad situations for the bot. It took me a couple attempts to get the data right. I kept labeling images as "OK" or "Safe" when the bot was about 1 bot length from colliding with something. This didn't leave the bot enough clearance to turn and it ended up colliding with the things. I found the sweet spot to be about 2 bot lengths from collision. After that, I was thoroughly impressed with the bot's ability to navigate a room.

I trained a basic AlexNet model on my laptop. You can run inference on these little GPUs but training deep learning models still requires more memory and compute than you have on these little boards. The model is a binary classification model that's being fed the video frames from the camera. I believe the frame rate is 30fps so that means the little GPU on the Jetson Nano board is making real-time predictions with a full-precision (i.e. 32-bit) model. It's doing that while running on a portable battery that's powering the board and the DC motors attached to the bot. That's pretty impressive in my book.
https://github.com/NVIDIA-AI-IOT/jetbot/blob/master/notebooks/collision_avoidance/train_model.ipynb

Next up was object tracking. This relied on a pre-trained single-shot detector (SSD) based on the Mobilenet v2 architecture trained on the COCO dataset. Nvidia did the hard work of converting the original Tensorflow model to their proprietary NVIDIA TensorRT model format. They quantized the model to half-precision (i.e. 16 bit float point weights) so that it will use less GPU memory and take advantage of low-level instructions that can execute floating point multiplications in fewer steps. TensorRT conversion also makes a number of other optimizations such as vertical and horizontal fusion. With vertical fusion, where possible, convolution, bias, and ReLU layers are fused to form a single layer. Horizontal fusion improves performance by combining layers that take the same source tensor and apply the same operations with similar parameters.
# insert photos https://docs.google.com/presentation/d/1v3Nb7qp_rJJj1pIZvrPC0sA7HV0zwd5FVj0bWZyKI80/edit#slide=id.g51ada02d8a_0_16
# https://github.com/NVIDIA-AI-IOT/jetbot/tree/master/jetbot/ssd_tensorrt

It's remarkable that we can now run deep learning models on edge devices. This is just scratching the surface of this world. If you're interested in learning more, I'd recommend checking out Pete Warden's blog. He's always posting about the latest and greatest in the AI edge computing space.

What do you think? Leave a comment at the link below.

